\documentclass[9pt,landscape]{article}
\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{color,graphicx,overpic}
\usepackage{hyperref}


\pdfinfo{
  /Title (example.pdf)
  /Creator (TeX)
  /Producer (pdfTeX 1.40.0)
  /Author (Seamus)
  /Subject (Example)
  /Keywords (pdflatex, latex,pdftex,tex)}

% This sets page margins to .5 inch if using letter paper, and to 1cm
% if using A4 paper. (This probably isn't strictly necessary.)
% If using another size paper, use default 1cm margins.
\ifthenelse{\lengthtest { \paperwidth = 11in}}
    { \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
    {\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
        {\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
        {\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
    }

% Turn off header and footer
\pagestyle{empty}

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother

% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}


\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}

%My Environments
\newtheorem{example}[section]{Example}
% -----------------------------------------------------------------------

\begin{document}
\raggedright
\footnotesize
\begin{multicols}{3}


% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\subsection{Parallel Job Scheduling}
Job collection of processes/threads that cooperate to solve some problem/service (not independent).\\
{\bf Space sharing} Divide processors into groups, assign job to dedicated set of processors, job waits until required number of CPUs are available (batch scheduling). Typically used on supercomputers.
Pros: 1) All runnable threads execute at the same time; 2) Reduce context switch overhead (no involuntary preemption); 3) Strong affinity. 
Cons: Inflexible: CPUs in one partition may be idle while another partition has multiple jobs waiting to run. Difficult to deal with dynamically-changing job sizes; Adaptive scheme is complicated and uncommon.
{\bf Scheduling convoy effect} In FCFS, long average wait times due to large job. Exists with FCFS uniprocessor batch systems, and much worse in parallel systems, leads to fragmentation of CPU space.
{\bf Backfilling}: Fill CPU “holes” from queue in FCFS order. Not FCFS anymore. Want to prevent ``fill'' from delaying threads that were in queue earlier.\\
{\bf Time sharing:} Each CPU may run threads from multiple jobs, but with awareness of jobs.
{\bf Gang Scheduling}: All-or-nothing, co-scheduled working set is all threads in the job. Get scheduling benefits of dedicated machine. Allows all jobs to get service. {\bf Issues:} All CPUs must context switch together. To avoid fragmentation, construct groups of jobs that fill a slot on each CPU. 8-CPU system, group one 4-thread job with two 2-thread jobs. Alternative 1: Paired gang scheduling - Identify groupings with complementary characteristics and pair them. When one blocks, the other runs. Alternative 2: Only use gang scheduling for thread groups that benefit. Fill holes in schedule with any single runnable thread from those remaining.\\
{\bf OS Noise} or how to schedule OS activities. Asynchronous OS activities perturb nice scheduling properties of running jobs together. Massively parallel systems are typically split into I/O nodes, management nodes, and compute nodes. Compute nodes are where the real work gets done. Run customized, lightweight kernel on compute nodes. Run full-blown OS on I/O nodes and mgmt nodes. {\bf Buffered coscheduling}: tolerate the noise by coscheduling the activities of the system software on each node.

\subsection{Virtual Memory}
{\bf Superpages:} page sizes are power-of-two multiples of the base page size.
Must be aligned in both virtual and physical memory (e.g. 4 MB superpage must begin on a 4 MB address boundary in both spaces).
TLB entry (copy of PTE) includes page size.
Why multiple superpage sizes? - Different apps have different “best” size.\\
{\bf The Superpage Problem:} allocation (what frame to choose on page fault), promotion (combine base pages into superpage), demotion (break superpage into smaller superpages, or base pages), fragmentation (need contiguous physical pages to create superpage).\\
{\bf Navarro et al' Design}
Key observation: once an application touches the first page of a memory object then it is likely that it will quickly touch every page of that object.
{\bf Allocation:} opportunistic, go for biggest size that is no larger than the memory object (e.g. file); if size not available, try preemption before resigning to a smaller size. Best candidate for preemption at front: reservation whose most recently populated frame was populated the least recently.
{\bf Promotion:} opportunistic, superpage is created whenever any superpage-sized and aligned extent within a reservation is fully populated.
{\bf Demotion:} 1) On memory pressure, demote superpages when resetting ref bit, help decide on eviction. Re-promote (incrementally) as pages are referenced. 2) Demote on first write to a clean superpage, only the written base pages are dirty. Re-promote (incrementally) as other pages are dirtied.
{\bf Fragmentation:} Superpages creation/allocation requires multiple contiguous free physical memory pages (equal to the superpage size). With multiple page sizes, the unit of allocation is no longer uniform, and “holes” of different sizes appear in the physical memory space. Eventually, there may be enough free memory for a process, but no contiguous chunks large enough to create the desired superpage size. Solution: 1) Restore contiguity: move clean, inactive pages to the free list. 2) Re-locating allocated pages to create larger free contiguous chunks (coalescing), and biasing page reclamation to prefer pages that restore the largest contiguous chunks 3) Clustering wired pages.\\
{\bf Direct-mapped} Each block can be stored in exactly 1 location in the cache. Mapping is (block address) modulo (num blocks in cache).
{\bf Fully associative} Any block can be stored in any cache line.
{\bf Set associative} Each block can be stored in a restricted set of locations in the cache.
Map block address to set first using (block addr) mod (num offsets), then place block within set. If N locations in a set, called N-way set associative.\\
{\bf What addr in cache?} {\bf Virtual} 1) 􏱃does not need to be translated before checking cache; 2) application programmer can reason about conflicts; 3) cache needs to flushed on context switch. {\bf Physical} 1) data may stay in cache across context switches; 2) vaddr must be translated before checking cache; 3) 􏱄conflicts depend on what physical page is allocated.\\
{\bf Conflict-aware page placement:} OS can select physical pages on allocation to try to reduce cache conflicts. Assign a colour to each page such that pages with different colours do not conflict in the cache. All pages with same colour map to same lines or sets in the cache. Number of colours = (cache size) / (pg size * associativity). A page's colour is (page number) modulo (num colours).
{\bf Page coloring:} Assign colour to virtual and physical pages. On page fault, allocate a physical page with the same colour as the virtual page. Exploits spatial locality. Programmer reasoning about virtual addresses still applies. OS keeps per-color free lists.
{\bf Bin hopping:} Assign colors to physical pages and keep per-colour free lists as before. On page fault, allocate physical page of next colour from last one previously allocated. Exploits temporal locality.

\subsection{Distributed Shared Memory}
Technique that allows distributed processes to transparently 
share a global virtual address space, although physical memory may be located on many nodes. 
Typically builds on existing virtual address translation hardware, 
augmented with software support for remote page fetch and consistency control.\\

\subsection{Distributed Systems}
{\bf Byzantine Failure} Failure model in which faulty process exhibits arbitrary behavior. 
Typically modelled as malicious attackers to capture worst case behavior; 
faulty processes may collude with other failed processes but are not more 
powerful than non-failed ones.\\

\subsection{Fault Tolerance and RSM}
{\bf Replicated State Machine:} A state machine consists of state variables and commands that modify state variables (causing state changes) and/or produce output. They are deterministic, so the same sequences of commands applied to the same state leads to the same output. Replicate the state machine on different servers. Clients interact with sets of servers.\\
Fault tolerant services are provided by having multiple replicas of the service all handle the same set of requests. As long as any instance is alive, the client can get a response (assuming fail-stop failures).\\
{\bf State Machine Commands:} A message that the state machine receives. 
Commands must execute atomically with respect to other commands (linearizability). Commands modify state variables and/or produce outputs. The state/output of a state machine is completely determined by the initial state and the sequence of commands.\\
{\bf RSM Failure} In the case of failures, clients must determine correct output of RSMs. RSMs are called $t$-tolerant: 1) Fail-stop: $t + 1$ replicas required (1 correct replica sufficient); 2) Byzantine: $2t + 1$ replicas required ($t + 1$ correct replicas sufficient). Different than Broadcast/Consensus failures - ine client must decide on result, replicas don't have to agree with each other about result.\\
Atomic broadcast is required for RSM because we must guarantee that all replicas process the same commands in exactly the same order, since all correct replicas must have the same state; otherwise they may not produce the same output. This is a form of distributed consensus.\\
{\bf Reliable Broadcast} (diffusion): all correct processes take on role of broadcaster upon receipt of message. Failures assumed to be fail-stop.
{\bf Properties of Send/Receive:}
1) Validity: If $p$ sends $m$ to $q$, and both $p$ and $q$ and the link between them are correct, then￼$q$ eventually receives $m$ (liveness). 2) Uniform Integrity: For any message $m$, $q$ receives $m$ at most once from $p$, and only if $p$ previously sent $m$ to $q$ (safety) e.g. Communication with TCP.
{\bf Properties of Broadcast/Deliver:}
1) Validity: If a correct process broadcasts a message $m$, then all correct processes eventually deliver $m$. 2) Agreement: If a correct process delivers a message $m$, then all correct processes eventually deliver $m$. 3) Integrity: For any message $m$, every correct process delivers $m$ at most once, and only if $m$ was previously broadcast by $send(m)$.\\
{\bf FIFO Order:} If a process broadcasts a message m before it broadcasts a message m', then no correct process delivers m' unless it has previously delivered m. 
{\bf Causal Order:} If the broadcast of a message m causally precedes the broadcast of a message m', then no correct process delivers m' unless it has previously delivered m. 
Total Order: All correct processes deliver messages in the same order.\\
{\bf FIFO broadcast:} layered on top of Reliable Broadcast. Each process $p$ maintains, for each other process $q_i$, the next sequence number it can $FIFODeliver$. Buffers $ReliableDeliver$'ed messages until the sequence number indicates message may be $FIFODeliver$'ed.\\
{\bf Causal broadcast} algorithm is layered on top of FIFO alg. $CausalBroadcast$ prepends list of messages upon which $m$ causally depends then calls $FIFOBroadcast$. Dependent messages is the list of messages $CausalDeliver$'ed since last $CausalBroadcast$. Buffers $FIFODeliver$'ed messages until all messages upon which $m$ depends have been $CausalDeliver$'ed.\\ 
{\bf Distributed Consensus:} Servers communicate amongst themselves to reach agreement on state.
{\bf Voting} Let V be the number of votes in the system; let W be the number of votes required to write; let R be the number of votes required to read. Overlap Constraint: $R + W > V$. Recommend: $2W > V$ and $R + W < V + \epsilon$. Data must contain a version number or timestamp. If constraints are met, then data will remain consistent. Votes can be arbitrarily assigned to servers in
the system (i.e. weights can be assigned to servers).
{\bf Quorums} are a generalization of voting, organize servers into logical structures, Overlap constraint: every write quorum must overlap with every read quorum (e.g. writes must go to a column, reads must get a row).

\subsection{Reliable Storage}
{\bf LFS Crash Recovery} LFS keeps two special “checkpoint regions” that record 1) the locations of the inode map blocks, 2) the segment usage table, 3) the address of the last block written in the log, and 4) a timestamp. On crash recovery, the checkpoint region with the most recent timestamp is used. The file system state (inode map and end of log) is consistent when the checkpoint is written, so we could start from that point, but modifications made between checkpoints would be lost. Instead, LFS rolls forward, reading the log from the end location recorded in the checkpoint and applying changes according to the operation log stored with each segment, thereby recovering a more up-to-date version of the file system while maintaining consistency.

\subsection{Security}
{\bf Vulnerability} A software flaw with a security implication.\\
{\bf Why buggy programs lead to compromised systems?} 
Certain classes of bugs (such as buffer overflows or format string vulnerabilities) result from improper handling of user input, and allow an attacker to gain control over the process running the buggy program. By itself, this would not lead to a compromised system, just a compromised process, but many OSs do not provide proper separation of authorization or privilege domains. (For example, setuid programs in Unix, or allowing any user to write the registry in Windows) In many cases, this means that a compromised process can in fact be used to compromise the system.\\
FreeBSD jails are an example of an OS security mechanism that addresses this problem. The idea is to give a process (and all its descendants) an isolated view of the system, including a limited view of the file system and its own superuser account, which can only affect things contained within the same jail.\\

% You can even have references
%\rule{0.3\linewidth}{0.25pt}
\scriptsize
\bibliographystyle{abstract}
\bibliography{refFile}
\end{multicols}
\end{document}
